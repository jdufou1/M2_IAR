{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279befaa-6225-4e13-a3b1-788d47d7c6f0",
   "metadata": {},
   "source": [
    "# Test environnement Tracking avec CACLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b7e43-88c7-465e-a1e0-418b27d54346",
   "metadata": {},
   "source": [
    "Reproduction de l'environnement de test pour le papier https://dspace.library.uu.nl/bitstream/handle/1874/25514/wiering_07_reinforcementlearning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fd7631-77d8-4e1e-a343-c99e625384b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    " \n",
    "# ajout de la classe Tracking\n",
    "sys.path.insert(0, '../')\n",
    "from Tracking import Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f3746-9b77-4fc0-a76f-e098c9ba782d",
   "metadata": {},
   "source": [
    "## hyper paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865bfd1-ac16-46dc-a3de-e7b1102b6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = 4\n",
    "action_space = 2\n",
    "\n",
    "discount_factor = 0.99\n",
    "learning_rate_critic =   2.5e-4  # 3e-2  \n",
    "learning_rate_actor =  8.5e-4\n",
    "sigma = [0.1,0.1]\n",
    "beta = 0.001\n",
    "var_0 = 1\n",
    "nb_episode = 2000\n",
    "test_frequency = 10\n",
    "nb_tests = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f9540-05ff-407f-bb1d-07d085940f4e",
   "metadata": {},
   "source": [
    "## Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592e869-7320-43b3-8501-862fee0efc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Tracking()\n",
    "env.reset()\n",
    "\n",
    "class ActorNetwork(nn.Module) :\n",
    "    \n",
    " \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_space, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class CriticNetwork(nn.Module) :\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_space, 12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "def test(actor) : \n",
    "    cum_rewards = list()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  actor(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        cum_rewards.append(reward)\n",
    "        state = new_state\n",
    "    return sum(cum_rewards) / len(cum_rewards)\n",
    "\n",
    "\n",
    "actor = ActorNetwork()\n",
    "critic = CriticNetwork()\n",
    "\n",
    "best_model = ActorNetwork()\n",
    "best_value = -1e10\n",
    "\n",
    "optimizer_critic = torch.optim.Adam(critic.parameters(),lr=learning_rate_critic)\n",
    "\n",
    "list_rewards_mean_4 = list()\n",
    "list_rewards_std_4 = list()\n",
    "\n",
    "for episode in tqdm(range(nb_episode)) : \n",
    "    \n",
    "    var = var_0\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    while not done :\n",
    "        \n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        \n",
    "        action = torch.as_tensor(np.array(np.random.normal(loc=actor(state_t).detach().numpy(),scale=sigma,size=(1,action_space))),dtype=torch.float32)[0].detach().numpy()\n",
    "       \n",
    "        new_state, reward, done = env.step(action)\n",
    "        \n",
    "        new_state_t = torch.as_tensor(new_state , dtype=torch.float32)\n",
    "        reward_t = torch.as_tensor(reward , dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            td_error = reward_t + discount_factor * (1 - done) * critic(new_state_t) - critic(state_t)\n",
    "        \n",
    "        # learning critic\n",
    "        loss_critic = - td_error.detach() * critic(state_t)\n",
    "\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        if td_error > 0 :\n",
    "            \n",
    "            optimizer_actor = torch.optim.Adam(actor.parameters(),lr=learning_rate_actor)\n",
    "            action_t = torch.as_tensor(action , dtype=torch.float32)\n",
    "            \n",
    "            # learning actor\n",
    "            loss_actor = ( (action_t - actor(state_t).detach()) * actor(state_t) ).mean()\n",
    "\n",
    "            optimizer_actor.zero_grad()\n",
    "            loss_actor.backward()\n",
    "            optimizer_actor.step()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "    if episode % test_frequency == 0 :    \n",
    "        list_tests = list()\n",
    "        for t in range(nb_tests) :\n",
    "            list_tests.append(test(actor))\n",
    "            \n",
    "        list_tests = np.array(list_tests)\n",
    "        \n",
    "        print(f\"test episode : {episode} - mean value : {list_tests.mean()} - best value : {best_value}\")\n",
    "\n",
    "        if list_tests.mean() > best_value :\n",
    "            best_value = list_tests.mean()\n",
    "            best_model.load_state_dict(actor.state_dict())\n",
    "        \n",
    "        list_rewards_mean_4.append( list_tests.mean() )\n",
    "        list_rewards_std_4.append( list_tests.std() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916782c7-dcb5-4ca2-958a-8ccf0feb83ba",
   "metadata": {},
   "source": [
    "## Affichage des rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052dcf17-3441-46aa-bb83-00e0f1a055b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "list_mean1_4 = np.array(list_rewards_mean_4) + np.array(list_rewards_std_4)\n",
    "list_mean2_4 = np.array(list_rewards_mean_4) - np.array(list_rewards_std_4)\n",
    "plt.fill_between(np.arange(0,nb_episode,test_frequency),list_mean1_4,list_mean2_4, color = 'salmon', label = 'std reward')\n",
    "plt.plot(np.arange(0,nb_episode,test_frequency),list_rewards_mean_4, c= 'r',label = 'mean reward')\n",
    "plt.legend()\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.title('Continuous Actor Critic (CACLA) - Tracking - Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87617e40-4739-49af-a975-7b0f743be4df",
   "metadata": {},
   "source": [
    "## Démonstration de la simulation de l'agent dans l'environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc0371-ea05-4997-91ab-86631c5fe0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "list_x_agent = list()\n",
    "list_y_agent = list()\n",
    "list_x_target = list()\n",
    "list_y_target = list()\n",
    "\n",
    "nb_iter = 0\n",
    "r = 0.0\n",
    "while not done :\n",
    "    state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "    action =  best_model(state_t).detach().numpy()\n",
    "    new_state, reward, done = env.step(action)\n",
    "    r += reward\n",
    "    list_x_agent.append(env.agent[0])\n",
    "    list_y_agent.append(env.agent[1])\n",
    "    list_x_target.append(env.target[0])\n",
    "    list_y_target.append(env.target[1])\n",
    "    state = new_state\n",
    "    nb_iter += 1\n",
    "    \n",
    "print(f\"iteration : {nb_iter}, reward : \",(r/300))\n",
    "plt.figure()\n",
    "plt.scatter(list_x_agent,list_y_agent , label=\"agent\")\n",
    "plt.scatter(list_x_target,list_y_target, label='target')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print((env.agent[0] - env.target[0])**2 + (env.agent[1] - env.target[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a850e-244c-47e5-bd18-11f16a2f18b3",
   "metadata": {},
   "source": [
    "## Etape par étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c90d4-8f19-46df-b38f-a622809bcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "list_x_agent = list()\n",
    "list_y_agent = list()\n",
    "list_x_target = list()\n",
    "list_y_target = list()\n",
    "\n",
    "iteration = 0\n",
    "while not done :\n",
    "    state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "    action =  best_model(state_t).detach().numpy()\n",
    "    new_state, reward, done = env.step(action)\n",
    "    state = new_state\n",
    "    iteration += 1\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    # plt.scatter(list_x_target[0],list_y_target[0], color=\"red\",label=\"first position target\")\n",
    "    plt.scatter(env.agent[0] ,env.agent[1], label=\"agent\")\n",
    "    plt.scatter(env.target[0],env.target[1], label='target')\n",
    "    rect=mpatches.Rectangle((4,5),5,1, \n",
    "                            fill=False,\n",
    "                            color=\"purple\",\n",
    "                           linewidth=2)\n",
    "                           #facecolor=\"red\")\n",
    "    plt.gca().add_patch(rect)\n",
    "    plt.xticks([0, 2, 4, 6, 8, 10])\n",
    "    plt.yticks([0, 2, 4, 6, 8, 10])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
