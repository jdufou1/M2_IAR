{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff3131d-1aba-4ae5-b94c-f9bbc9cb3a7b",
   "metadata": {},
   "source": [
    "Ce notebook permet de comparer les résultats des algorithmes **CAC**,**CACLA** et **CACLA+VAR** avec une stratégie d'exploration gaussienne et $\\epsilon$-greedy sur l'environnement **Tracking** du papier  https://www.researchgate.net/publication/4249966_Reinforcement_Learning_in_Continuous_Action_Spaces/link/0912f5093a214c7f1b000000/download\n",
    "\n",
    "Fait par Jérémy DUFOURMANTELLE et Ethan ABITBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218e760-3504-4bbf-83e1-b89ba69a98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils.Tracking import Tracking\n",
    "from utils.Critic import CriticNetwork\n",
    "from utils.Actor import ActorNetwork\n",
    "from utils.CAC import CAC\n",
    "from utils.CACLA import CACLA\n",
    "from utils.CACLAVAR import CACLAVAR\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a52176-3507-47a6-ade5-3145ef28340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tests_global = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc1f29-9ce5-43b0-a068-6c0d4d46a628",
   "metadata": {},
   "source": [
    "# Gaussian Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a5eed-f6c6-406e-bce0-461cdb716a40",
   "metadata": {},
   "source": [
    "### CAC with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0c013-c20a-4fdd-9f7a-67a84dea0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cac = 0\n",
    "fails_cac = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cac = list()\n",
    "matrice_simulation_iteration_cac = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cac = CAC(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cac.learning()\n",
    "    matrice_simulation_rewards_cac.append(cac.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cac.append(cac.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cac.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cac += 1\n",
    "    else :\n",
    "        success_cac += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c352229-bddf-4027-83d6-335e1e920f02",
   "metadata": {},
   "source": [
    "### CACLA with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21cc36-744f-4fc3-a6be-d49794a4b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cacla = 0\n",
    "fails_cacla = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cacla = list()\n",
    "matrice_simulation_iteration_cacla = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cacla = CACLA(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.95,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    \n",
    "    cacla.learning()\n",
    "    matrice_simulation_rewards_cacla.append(cacla.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cacla.append(cacla.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cacla.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cacla += 1\n",
    "    else :\n",
    "        success_cacla += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f9231-bbf3-498a-9152-b37336092838",
   "metadata": {},
   "source": [
    "### CACLA+VAR with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0f226-e5bf-4bd1-a042-38e8fc09405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_caclavar = 0\n",
    "fails_caclavar = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_caclavar = list()\n",
    "matrice_simulation_iteration_caclavar = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    caclavar = CACLAVAR(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.8,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    caclavar.learning()\n",
    "    matrice_simulation_rewards_caclavar.append(caclavar.list_rewards_mean)\n",
    "    matrice_simulation_iteration_caclavar.append(caclavar.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  caclavar.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_caclavar += 1\n",
    "    else :\n",
    "        success_caclavar += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08c499-36bf-47f8-aafc-f0dfa462f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[gaussian] Nombre de tests : {nb_tests}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CAC: {success_cac}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CAC: {fails_cac}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CAC: {success_cac/(success_cac+fails_cac)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CACLA: {success_cacla}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CACLA: {fails_cacla}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CACLA: {success_cacla/(success_cacla+fails_cacla)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CACLAVAR: {success_caclavar}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CACLAVAR: {fails_caclavar}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CACLAVAR: {success_caclavar/(success_caclavar+fails_caclavar)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14a69a-2ac9-444c-9ed2-011b76255dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_normalization(matrice_simulation_rewards) :\n",
    "    dist_max = -200\n",
    "    dist_min = 0\n",
    "    arr = np.array(matrice_simulation_rewards)\n",
    "    return 1 - ( arr / (dist_max - dist_min)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27a8c8-8c09-4d52-9300-c6083a86a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cac = rewards_normalization(matrice_simulation_rewards_cac)\n",
    "l_cacla = rewards_normalization(matrice_simulation_rewards_cacla)\n",
    "l_caclavar = rewards_normalization(matrice_simulation_rewards_caclavar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d6867-d2dd-4fac-aac9-65639f3795e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_iter_caclavar = np.array(matrice_simulation_iteration_caclavar)\n",
    "x_iter_caclavar = m_iter_caclavar.mean(axis=0)\n",
    "\n",
    "m_iter_cacla = np.array(matrice_simulation_iteration_cacla)\n",
    "x_iter_cacla = m_iter_cacla.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957000ff-6dce-47be-bf0b-f74cee966477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cac, label=\"rewards CAC\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,11))\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on Tracking\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(x_iter_cacla,l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(x_iter_caclavar,l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.xticks([0,51200,102400])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4f196-6f96-4759-bb15-b38aa0def4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultats : \")\n",
    "print(\"[gaussian] CAC : mean rewards -> \",l_cac.mean())\n",
    "print(\"[gaussian] CAC : std rewards -> \",l_cac.std())\n",
    "print(\"[gaussian] CACLA : mean rewards -> \",l_cacla.mean())\n",
    "print(\"[gaussian] CACLA : std rewards -> \",l_cacla.std())\n",
    "print(\"[gaussian] CACLAVAR : mean rewards -> \",l_caclavar.mean())\n",
    "print(\"[gaussian] CACLAVAR : std rewards -> \",l_caclavar.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426298e-5ec0-4c3a-b338-c863cfea4f10",
   "metadata": {},
   "source": [
    "# $\\epsilon$-greedy Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1760f6-6d8c-4d18-8c29-f300df1137d3",
   "metadata": {},
   "source": [
    "### CAC with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c098b10-9cc6-4781-a24d-8a415edaa158",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cac = 0\n",
    "fails_cac = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cac = list()\n",
    "matrice_simulation_iteration_cac = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cac = CAC(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"egreedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cac.learning()\n",
    "    matrice_simulation_rewards_cac.append(cac.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cac.append(cac.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cac.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cac += 1\n",
    "    else :\n",
    "        success_cac += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0e905-94c7-424e-a014-c27a4982b3ba",
   "metadata": {},
   "source": [
    "### CACLA with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c51537-b755-4541-aa0e-0c0edf29fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cacla = 0\n",
    "fails_cacla = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cacla = list()\n",
    "matrice_simulation_iteration_cacla = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cacla = CACLA(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.95,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"egreedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cacla.learning()\n",
    "    matrice_simulation_rewards_cacla.append(cacla.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cacla.append(cacla.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cacla.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cacla += 1\n",
    "    else :\n",
    "        success_cacla += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866d245-9f56-45ef-83f8-4a592c553d62",
   "metadata": {},
   "source": [
    "### CACLA+VAR with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061d2a0-2b94-480b-97ce-1e2edc4f1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_caclavar = 0\n",
    "fails_caclavar = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_caclavar = list()\n",
    "matrice_simulation_iteration_caclavar = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    caclavar = CACLAVAR(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"egreedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    caclavar.learning()\n",
    "    matrice_simulation_rewards_caclavar.append(caclavar.list_rewards_mean)\n",
    "    matrice_simulation_iteration_caclavar.append(caclavar.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  caclavar.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_caclavar += 1\n",
    "    else :\n",
    "        success_caclavar += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7615d1f-3272-4576-a794-ce4cf7b6c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[egreedy] Nombre de tests : {nb_tests}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CAC: {success_cac}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CAC: {fails_cac}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CAC: {success_cac/(success_cac+fails_cac)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CACLA: {success_cacla}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CACLA: {fails_cacla}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CACLA: {success_cacla/(success_cacla+fails_cacla)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CACLAVAR: {success_caclavar}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CACLAVAR: {fails_caclavar}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CACLAVAR: {success_caclavar/(success_caclavar+fails_caclavar)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035235fd-0936-4f44-9627-a973a683a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_normalization(matrice_simulation_rewards) :\n",
    "    dist_max = -200\n",
    "    dist_min = 0\n",
    "    arr = np.array(matrice_simulation_rewards)\n",
    "    return 1 - ( arr / (dist_max - dist_min)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de902cd6-63b8-42a5-a68e-42d978f37377",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cac = rewards_normalization(matrice_simulation_rewards_cac)\n",
    "l_cacla = rewards_normalization(matrice_simulation_rewards_cacla)\n",
    "l_caclavar = rewards_normalization(matrice_simulation_rewards_caclavar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94565f91-c7a2-423a-bc70-73dc9b0b5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_iter_caclavar = np.array(matrice_simulation_iteration_caclavar)\n",
    "x_iter_caclavar = m_iter_caclavar.mean(axis=0)\n",
    "\n",
    "m_iter_cacla = np.array(matrice_simulation_iteration_cacla)\n",
    "x_iter_cacla = m_iter_cacla.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf886b-5cd3-4e91-9e0c-756e67f9bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cac, label=\"rewards CAC\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,11))\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on Tracking\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(x_iter_cacla,l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(x_iter_caclavar,l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.xticks([0,51200,102400])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada48e0-d266-4c63-9f07-81198f873810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultats : \")\n",
    "print(\"[egreedy] CAC : mean rewards -> \",l_cac.mean())\n",
    "print(\"[egreedy] CAC : std rewards -> \",l_cac.std())\n",
    "print(\"[egreedy] CACLA : mean rewards -> \",l_cacla.mean())\n",
    "print(\"[egreedy] CACLA : std rewards -> \",l_cacla.std())\n",
    "print(\"[egreedy] CACLAVAR : mean rewards -> \",l_caclavar.mean())\n",
    "print(\"[egreedy] CACLAVAR : std rewards -> \",l_caclavar.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
