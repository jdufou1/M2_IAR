{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279befaa-6225-4e13-a3b1-788d47d7c6f0",
   "metadata": {},
   "source": [
    "# Test environnement Tracking avec CACLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b7e43-88c7-465e-a1e0-418b27d54346",
   "metadata": {},
   "source": [
    "Reproduction de l'environnement de test pour le papier https://dspace.library.uu.nl/bitstream/handle/1874/25514/wiering_07_reinforcementlearning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f92839-0049-44bd-a903-cb55fb329d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils.Tracking import Tracking\n",
    "from utils.Critic import CriticNetwork\n",
    "from utils.Actor import ActorNetwork\n",
    "from utils.CACLA import CACLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f3746-9b77-4fc0-a76f-e098c9ba782d",
   "metadata": {},
   "source": [
    "## hyper paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c02b06-2a74-4de3-8146-7facc2ba8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Tracking()\n",
    "actor_network = ActorNetwork(\n",
    "    nb_neurons = 12,\n",
    "    action_space = env.action_space,\n",
    "    observation_space = env.observation_space\n",
    ")\n",
    "\n",
    "critic_network = CriticNetwork(\n",
    "    nb_neurons = 12,\n",
    "    observation_space = env.observation_space\n",
    ")\n",
    "\n",
    "cacla = CACLA(\n",
    "    learning_rate_critic = 0.01,\n",
    "    learning_rate_actor = 0.01,\n",
    "    discount_factor = 0.95,\n",
    "    sigma = 0.1,\n",
    "    nb_episode = 500,\n",
    "    test_frequency = 10,\n",
    "    env = env,\n",
    "    actor_network = actor_network,\n",
    "    critic_network = critic_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f9540-05ff-407f-bb1d-07d085940f4e",
   "metadata": {},
   "source": [
    "## Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760a785-d04e-47a7-91e8-e289dae4bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacla.learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916782c7-dcb5-4ca2-958a-8ccf0feb83ba",
   "metadata": {},
   "source": [
    "## Affichage des rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad50d6-62ce-47c6-bc1a-558d2d5a78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(cacla.list_rewards, c= 'r',label = 'rewards')\n",
    "plt.legend()\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.title('Continuous Actor Critic (CACLA) - Tracking - Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87617e40-4739-49af-a975-7b0f743be4df",
   "metadata": {},
   "source": [
    "## Démonstration de la simulation de l'agent dans l'environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20fd63-65a7-4238-97f1-aef6193a46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "list_x_agent = list()\n",
    "list_y_agent = list()\n",
    "list_x_target = list()\n",
    "list_y_target = list()\n",
    "\n",
    "nb_iter = 0\n",
    "r = 0.0\n",
    "list_x_agent.append(env.agent[0])\n",
    "list_y_agent.append(env.agent[1])\n",
    "list_x_target.append(env.target[0])\n",
    "list_y_target.append(env.target[1])\n",
    "while not done :\n",
    "    state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "    action =  cacla.best_model(state_t).detach().numpy()\n",
    "    new_state, reward, done = env.step(action)\n",
    "    r += reward\n",
    "    list_x_agent.append(env.agent[0])\n",
    "    list_y_agent.append(env.agent[1])\n",
    "    list_x_target.append(env.target[0])\n",
    "    list_y_target.append(env.target[1])\n",
    "    state = new_state\n",
    "    nb_iter += 1\n",
    "    \n",
    "print(f\"iteration : {nb_iter}, reward : \",(r/300))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(list_x_agent,list_y_agent , label=\"agent\")\n",
    "plt.scatter(list_x_target,list_y_target, label='target')\n",
    "rect=mpatches.Rectangle((4,5),5,1, \n",
    "                            fill=False,\n",
    "                            color=\"purple\",\n",
    "                           linewidth=2)\n",
    "                           #facecolor=\"red\")\n",
    "plt.gca().add_patch(rect)\n",
    "plt.xticks([0, 2, 4, 6, 8, 10])\n",
    "plt.yticks([0, 2, 4, 6, 8, 10])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Tracking simulation with the agent's policy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print((env.agent[0] - env.target[0])**2 + (env.agent[1] - env.target[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a850e-244c-47e5-bd18-11f16a2f18b3",
   "metadata": {},
   "source": [
    "## Etape par étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb573ac2-10fc-4943-9f5e-5f5e9cf9f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "list_x_agent = list()\n",
    "list_y_agent = list()\n",
    "list_x_target = list()\n",
    "list_y_target = list()\n",
    "\n",
    "iteration = 0\n",
    "while not done :\n",
    "    state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "    action =  cacla.best_model(state_t).detach().numpy()\n",
    "    new_state, reward, done = env.step(action)\n",
    "    state = new_state\n",
    "    iteration += 1\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    # plt.scatter(list_x_target[0],list_y_target[0], color=\"red\",label=\"first position target\")\n",
    "    plt.scatter(env.agent[0] ,env.agent[1], label=\"agent\")\n",
    "    plt.scatter(env.target[0],env.target[1], label='target')\n",
    "    rect=mpatches.Rectangle((4,5),5,1, \n",
    "                            fill=False,\n",
    "                            color=\"purple\",\n",
    "                           linewidth=2)\n",
    "                           #facecolor=\"red\")\n",
    "    plt.gca().add_patch(rect)\n",
    "    plt.xticks([0, 2, 4, 6, 8, 10])\n",
    "    plt.yticks([0, 2, 4, 6, 8, 10])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
