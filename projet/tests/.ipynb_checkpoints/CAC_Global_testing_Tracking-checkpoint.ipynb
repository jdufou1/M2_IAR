{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff3131d-1aba-4ae5-b94c-f9bbc9cb3a7b",
   "metadata": {},
   "source": [
    "Ce notebook permet de comparer les résultats des algorithmes **CAC**,**CACLA** et **CACLA+VAR** avec une stratégie d'exploration gaussienne et $\\epsilon$-greedy sur l'environnement **Tracking** du papier  https://www.researchgate.net/publication/4249966_Reinforcement_Learning_in_Continuous_Action_Spaces/link/0912f5093a214c7f1b000000/download\n",
    "\n",
    "Fait par Jérémy DUFOURMANTELLE et Ethan ABITBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec93ba91-7d51-4134-af60-79ec53aa133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils.Tracking import Tracking\n",
    "from utils.Critic import CriticNetwork\n",
    "from utils.Actor import ActorNetwork\n",
    "from utils.CAC import CAC\n",
    "from utils.CACLA import CACLA\n",
    "from utils.CACLAVAR import CACLAVAR\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc1f29-9ce5-43b0-a068-6c0d4d46a628",
   "metadata": {},
   "source": [
    "# Gaussian Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a5eed-f6c6-406e-bce0-461cdb716a40",
   "metadata": {},
   "source": [
    "### CAC with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41328cf4-e38a-424e-968a-16206d7fa9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cac = 0\n",
    "fails_cac = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_cac = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cac = CAC(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cac.learning()\n",
    "    matrice_simulation_rewards_cac.append(cac.list_rewards_mean)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cac.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cac += 1\n",
    "    else :\n",
    "        success_cac += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c352229-bddf-4027-83d6-335e1e920f02",
   "metadata": {},
   "source": [
    "### CACLA with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505d3c3-470f-4f05-8c76-ba17f7dc2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cacla = 0\n",
    "fails_cacla = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_cacla = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cacla = CACLA(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.95,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cacla.learning()\n",
    "    matrice_simulation_rewards_cacla.append(cacla.list_rewards_mean)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cacla.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cacla += 1\n",
    "    else :\n",
    "        success_cacla += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f9231-bbf3-498a-9152-b37336092838",
   "metadata": {},
   "source": [
    "### CACLA+VAR with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c602e0-ce70-4bbb-9b62-17b08676e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_caclavar = 0\n",
    "fails_caclavar = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_caclavar = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    caclavar = CACLAVAR(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.8,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    caclavar.learning()\n",
    "    matrice_simulation_rewards_caclavar.append(caclavar.list_rewards_mean)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  caclavar.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_caclavar += 1\n",
    "    else :\n",
    "        success_caclavar += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af99b21-a6f2-4d60-a98d-3a783d5aacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[gaussian] Nombre de tests : {nb_tests}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CAC: {success_cac}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CAC: {fails_cac}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CAC: {success_cac/(success_cac+fails_cac)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CACLA: {success_cacla}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CACLA: {fails_cacla}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CACLA: {success_cacla/(success_cacla+fails_cacla)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CACLAVAR: {success_caclavar}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CACLAVAR: {fails_caclavar}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CACLAVAR: {success_caclavar/(success_caclavar+fails_caclavar)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6085708-afe6-4202-b35e-8b32086ded13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_normalization(matrice_simulation_rewards) :\n",
    "    dist_max = -200\n",
    "    dist_min = 0\n",
    "    arr = np.array(matrice_simulation_rewards)\n",
    "    return 1 - ( arr / (dist_max - dist_min)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2f197-e0a9-4af3-97c9-01fc3429f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cac = rewards_normalization(matrice_simulation_rewards)\n",
    "l_cacla = rewards_normalization(matrice_simulation_rewards)\n",
    "l_caclavar = rewards_normalization(matrice_simulation_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412220ee-2606-4109-aa84-a902aeb523ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cac, label=\"rewards CAC\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf48ad7-aba5-4035-82b9-633eb3eb1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultats : \")\n",
    "print(\"[gaussian] CAC : mean rewards -> \",l_cac.mean())\n",
    "print(\"[gaussian] CAC : std rewards -> \",l_cac.std())\n",
    "print(\"[gaussian] CACLA : mean rewards -> \",l_cacla.mean())\n",
    "print(\"[gaussian] CACLA : std rewards -> \",l_cacla.std())\n",
    "print(\"[gaussian] CACLAVAR : mean rewards -> \",l_caclavar.mean())\n",
    "print(\"[gaussian] CACLAVAR : std rewards -> \",l_caclavar.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426298e-5ec0-4c3a-b338-c863cfea4f10",
   "metadata": {},
   "source": [
    "# $\\epsilon$-greedy Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1760f6-6d8c-4d18-8c29-f300df1137d3",
   "metadata": {},
   "source": [
    "### CAC with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752713a3-f273-4ae6-a98d-4953d9633232",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cac = 0\n",
    "fails_cac = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_cac = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cac = CAC(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"egreedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cac.learning()\n",
    "    matrice_simulation_rewards_cac.append(cac.list_rewards_mean)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cac.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cac += 1\n",
    "    else :\n",
    "        success_cac += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0e905-94c7-424e-a014-c27a4982b3ba",
   "metadata": {},
   "source": [
    "### CACLA with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fd056-eafe-463f-b21c-b81b3d8a567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cacla = 0\n",
    "fails_cacla = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_cacla = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cacla = CACLA(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.95,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"greedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cacla.learning()\n",
    "    matrice_simulation_rewards_cacla.append(cacla.list_rewards_mean)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cacla.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cacla += 1\n",
    "    else :\n",
    "        success_cacla += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866d245-9f56-45ef-83f8-4a592c553d62",
   "metadata": {},
   "source": [
    "### CACLA+VAR with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769f568-a42e-4d64-84a0-bb4359b43833",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_caclavar = 0\n",
    "fails_caclavar = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_caclavar = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    caclavar = CACLAVAR(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"greedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    caclavar.learning()\n",
    "    matrice_simulation_rewards_caclavar.append(caclavar.list_rewards_mean)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  caclavar.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_caclavar += 1\n",
    "    else :\n",
    "        success_caclavar += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2a94e-da59-4789-915b-8369af7bfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[egreedy] Nombre de tests : {nb_tests}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CAC: {success_cac}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CAC: {fails_cac}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CAC: {success_cac/(success_cac+fails_cac)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CACLA: {success_cacla}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CACLA: {fails_cacla}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CACLA: {success_cacla/(success_cacla+fails_cacla)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CACLAVAR: {success_caclavar}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CACLAVAR: {fails_caclavar}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CACLAVAR: {success_caclavar/(success_caclavar+fails_caclavar)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cee68-bddc-4795-b3c7-7a2c5bcc0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_normalization(matrice_simulation_rewards) :\n",
    "    dist_max = -200\n",
    "    dist_min = 0\n",
    "    arr = np.array(matrice_simulation_rewards)\n",
    "    return 1 - ( arr / (dist_max - dist_min)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13665912-6ae2-48f9-a6ff-28ae4752e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cac = rewards_normalization(matrice_simulation_rewards)\n",
    "l_cacla = rewards_normalization(matrice_simulation_rewards)\n",
    "l_caclavar = rewards_normalization(matrice_simulation_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d3498-5c2b-4be8-8642-863c2769715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cac, label=\"rewards CAC\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162ce6f-b913-4c1f-b05e-838216f8dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultats : \")\n",
    "print(\"[egreedy] CAC : mean rewards -> \",l_cac.mean())\n",
    "print(\"[egreedy] CAC : std rewards -> \",l_cac.std())\n",
    "print(\"[egreedy] CACLA : mean rewards -> \",l_cacla.mean())\n",
    "print(\"[egreedy] CACLA : std rewards -> \",l_cacla.std())\n",
    "print(\"[egreedy] CACLAVAR : mean rewards -> \",l_caclavar.mean())\n",
    "print(\"[egreedy] CACLAVAR : std rewards -> \",l_caclavar.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
