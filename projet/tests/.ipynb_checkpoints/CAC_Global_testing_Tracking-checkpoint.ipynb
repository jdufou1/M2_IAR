{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93ba91-7d51-4134-af60-79ec53aa133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils.Tracking import Tracking\n",
    "from utils.Critic import CriticNetwork\n",
    "from utils.Actor import ActorNetwork\n",
    "from utils.CAC import CAC\n",
    "from utils.CACLA import CACLA\n",
    "from utils.CACLAVAR import CACLAVAR\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655cdea-f99e-4236-b97d-3f89ead2c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cac = 0\n",
    "fails_cac = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_cac = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cac = CAC(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cac.learning()\n",
    "    matrice_simulation_rewards_cac.append(cac.list_rewards)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cac.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cac += 1\n",
    "    else :\n",
    "        success_cac += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505d3c3-470f-4f05-8c76-ba17f7dc2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cacla = 0\n",
    "fails_cacla = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_cacla = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cacla = CACLA(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.95,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cacla.learning()\n",
    "    matrice_simulation_rewards_cacla.append(cacla.list_rewards)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cacla.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cacla += 1\n",
    "    else :\n",
    "        success_cacla += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c602e0-ce70-4bbb-9b62-17b08676e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_caclavar = 0\n",
    "fails_caclavar = 0\n",
    "nb_tests = 50\n",
    "\n",
    "matrice_simulation_rewards_caclavar = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = Tracking()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    caclavar = CACLAVAR(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.8,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    caclavar.learning()\n",
    "    matrice_simulation_rewards_caclavar.append(caclavar.list_rewards)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  caclavar.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_caclavar += 1\n",
    "    else :\n",
    "        success_caclavar += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af99b21-a6f2-4d60-a98d-3a783d5aacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de tests : {nb_tests}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Nombre de succes CAC: {success_cac}\")\n",
    "print(f\"Nombre d'echecs CAC: {fails_cac}\")\n",
    "print(f\"Ratio de succes pour CAC: {success_cac/(success_cac+fails_cac)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Nombre de succes CACLA: {success_cacla}\")\n",
    "print(f\"Nombre d'echecs CACLA: {fails_cacla}\")\n",
    "print(f\"Ratio de succes pour CACLA: {success_cacla/(success_cacla+fails_cacla)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Nombre de succes CACLAVAR: {success_caclavar}\")\n",
    "print(f\"Nombre d'echecs CACLAVAR: {fails_caclavar}\")\n",
    "print(f\"Ratio de succes pour CACLAVAR: {success_caclavar/(success_caclavar+fails_caclavar)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6085708-afe6-4202-b35e-8b32086ded13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_normalization(matrice_simulation_rewards) :\n",
    "    dist_max = -200\n",
    "    dist_min = 0\n",
    "    arr = np.array(matrice_simulation_rewards)\n",
    "    return 1 - ( arr / (dist_max - dist_min)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2f197-e0a9-4af3-97c9-01fc3429f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cac = rewards_normalization(matrice_simulation_rewards)\n",
    "l_cacla = rewards_normalization(matrice_simulation_rewards)\n",
    "l_caclavar = rewards_normalization(matrice_simulation_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412220ee-2606-4109-aa84-a902aeb523ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations on Tracking\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cac, label=\"rewards CAC\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf48ad7-aba5-4035-82b9-633eb3eb1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultats : \")\n",
    "print(\"CAC : mean rewards -> \",l_cac.mean())\n",
    "print(\"CAC : std rewards -> \",l_cac.std())\n",
    "print(\"CACLA : mean rewards -> \",l_cacla.mean())\n",
    "print(\"CACLA : std rewards -> \",l_cacla.std())\n",
    "print(\"CACLAVAR : mean rewards -> \",l_caclavar.mean())\n",
    "print(\"CACLAVAR : std rewards -> \",l_caclavar.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
