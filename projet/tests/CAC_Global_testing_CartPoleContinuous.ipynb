{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703a272e-a83e-4b4c-983a-ae3060ef8b91",
   "metadata": {},
   "source": [
    "Ce notebook permet de comparer les résultats des algorithmes **CAC**,**CACLA** et **CACLA+VAR** avec une stratégie d'exploration gaussienne et $\\epsilon$-greedy sur l'environnement **CartPole Continuous** du papier  https://www.researchgate.net/publication/4249966_Reinforcement_Learning_in_Continuous_Action_Spaces/link/0912f5093a214c7f1b000000/download\n",
    "\n",
    "Fait par Jérémy DUFOURMANTELLE et Ethan ABITBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88be3e-3fa6-4d44-a0b8-10e9643105d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils.CartPoleContinuous import ContinuousCartPoleEnv\n",
    "from utils.Critic import CriticNetwork\n",
    "from utils.Actor import ActorNetwork\n",
    "from utils.CAC import CAC\n",
    "from utils.CACLA import CACLA\n",
    "from utils.CACLAVAR import CACLAVAR\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a7cb3-7d29-4185-b4b6-8e6082a92a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tests_global = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c015479-3299-486f-aa17-dadbae8c2740",
   "metadata": {},
   "source": [
    "# Gaussian Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2211525-8759-416a-bf15-78f51c0ee384",
   "metadata": {},
   "source": [
    "### CAC with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c7682-32c8-4a27-bbce-932bc425fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cac = 0\n",
    "fails_cac = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cac = list()\n",
    "matrice_simulation_iteration_cac = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = ContinuousCartPoleEnv()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cac = CAC(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 50000,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cac.learning()\n",
    "    matrice_simulation_rewards_cac.append(cac.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cac.append(cac.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cac.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cac += 1\n",
    "    else :\n",
    "        success_cac += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e760e9-b470-41ef-8d01-519ca12722ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_simulation_iteration_cac = np.array(matrice_simulation_iteration_cac)\n",
    "print(matrice_simulation_iteration_cac.max())\n",
    "matrice_simulation_rewards_cac = np.array(matrice_simulation_rewards_cac)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(matrice_simulation_rewards_cac.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f04c09-8848-4ef7-be50-73917b7ecdbe",
   "metadata": {},
   "source": [
    "### CACLA with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b954913-8a6f-4316-a232-ff14882fd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cacla = 0\n",
    "fails_cacla = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cacla = list()\n",
    "matrice_simulation_iteration_cacla = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = ContinuousCartPoleEnv()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cacla = CACLA(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.8,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 50000,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    \n",
    "    cacla.learning()\n",
    "    matrice_simulation_rewards_cacla.append(cacla.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cacla.append(cacla.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cacla.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cacla += 1\n",
    "    else :\n",
    "        success_cacla += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a37494-ec09-477d-9567-38b0482830dc",
   "metadata": {},
   "source": [
    "### CACLA+VAR with gaussian exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60891e61-859e-4cb9-9993-e53ada2817db",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_caclavar = 0\n",
    "fails_caclavar = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_caclavar = list()\n",
    "matrice_simulation_iteration_caclavar = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = ContinuousCartPoleEnv()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    caclavar = CACLAVAR(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.95,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 50000,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"gaussian\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    caclavar.learning()\n",
    "    matrice_simulation_rewards_caclavar.append(caclavar.list_rewards_mean)\n",
    "    matrice_simulation_iteration_caclavar.append(caclavar.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  caclavar.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_caclavar += 1\n",
    "    else :\n",
    "        success_caclavar += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f011633-5ddd-412b-b917-a56bf34fa1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[gaussian] Nombre de tests : {nb_tests}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CAC: {success_cac}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CAC: {fails_cac}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CAC: {success_cac/(success_cac+fails_cac)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CACLA: {success_cacla}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CACLA: {fails_cacla}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CACLA: {success_cacla/(success_cacla+fails_cacla)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[gaussian] Nombre de succes CACLAVAR: {success_caclavar}\")\n",
    "print(f\"[gaussian] Nombre d'echecs CACLAVAR: {fails_caclavar}\")\n",
    "print(f\"[gaussian] Ratio de succes pour CACLAVAR: {success_caclavar/(success_caclavar+fails_caclavar)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940bd74-3239-4c7e-b4b2-5b94ae1bb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_normalization(matrice_simulation_rewards) :\n",
    "    dist_max = -200\n",
    "    dist_min = 0\n",
    "    arr = np.array(matrice_simulation_rewards)\n",
    "    # return 1 - ( arr / (dist_max - dist_min)).mean(axis=0)\n",
    "    return arr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1991f-0f35-4b99-be59-247e79c4ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cac = rewards_normalization(matrice_simulation_rewards_cac)\n",
    "l_cacla = rewards_normalization(matrice_simulation_rewards_cacla)\n",
    "l_caclavar = rewards_normalization(matrice_simulation_rewards_caclavar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0bba04-1153-492d-b5a1-355961889f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_iter_caclavar = np.array(matrice_simulation_iteration_caclavar)\n",
    "x_iter_caclavar = m_iter_caclavar.mean(axis=0)\n",
    "\n",
    "m_iter_cacla = np.array(matrice_simulation_iteration_cacla)\n",
    "x_iter_cacla = m_iter_cacla.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00774b3-6890-42b6-881d-e517c255f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on CartPole Continuous\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cac, label=\"rewards CAC\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on CartPole Continuous\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,11))\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and gaussian exploration on CartPole Continuous\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(x_iter_cacla,l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(x_iter_caclavar,l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.xticks([0,51200,102400])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e75425-238d-4167-b19a-c0d99b20fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultats : \")\n",
    "print(\"[gaussian] CAC : mean rewards -> \",l_cac.mean())\n",
    "print(\"[gaussian] CAC : std rewards -> \",l_cac.std())\n",
    "print(\"[gaussian] CACLA : mean rewards -> \",l_cacla.mean())\n",
    "print(\"[gaussian] CACLA : std rewards -> \",l_cacla.std())\n",
    "print(\"[gaussian] CACLAVAR : mean rewards -> \",l_caclavar.mean())\n",
    "print(\"[gaussian] CACLAVAR : std rewards -> \",l_caclavar.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5ebeb-e5ef-4b43-941c-ff1c18800d7b",
   "metadata": {},
   "source": [
    "# $\\epsilon$-greedy Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09a575-2231-4249-88e0-5d4a983e73cf",
   "metadata": {},
   "source": [
    "### CAC with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba1f0d-260d-4609-a969-ea9d2b831398",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cac = 0\n",
    "fails_cac = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cac = list()\n",
    "matrice_simulation_iteration_cac = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = ContinuousCartPoleEnv()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cac = CAC(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.9,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"egreedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cac.learning()\n",
    "    matrice_simulation_rewards_cac.append(cac.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cac.append(cac.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cac.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cac += 1\n",
    "    else :\n",
    "        success_cac += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbaf043-168d-40e9-a4fc-d555b15bb060",
   "metadata": {},
   "source": [
    "### CACLA with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33cbd3-55f6-4908-bd59-5ab3ac59b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cacla = 0\n",
    "fails_cacla = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_cacla = list()\n",
    "matrice_simulation_iteration_cacla = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = ContinuousCartPoleEnv()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    cacla = CACLA(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.95,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"egreedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    cacla.learning()\n",
    "    matrice_simulation_rewards_cacla.append(cacla.list_rewards_mean)\n",
    "    matrice_simulation_iteration_cacla.append(cacla.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  cacla.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_cacla += 1\n",
    "    else :\n",
    "        success_cacla += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354cfbf-cc6a-4a3f-8f38-75537b1c791f",
   "metadata": {},
   "source": [
    "### CACLA+VAR with $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff600395-ba50-4ca7-bdc8-b6bb8c63d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_caclavar = 0\n",
    "fails_caclavar = 0\n",
    "nb_tests = nb_tests_global\n",
    "\n",
    "matrice_simulation_rewards_caclavar = list()\n",
    "matrice_simulation_iteration_caclavar = list()\n",
    "\n",
    "for i in tqdm(range(nb_tests)) : \n",
    "    env = ContinuousCartPoleEnv()\n",
    "    actor_network = ActorNetwork(\n",
    "        nb_neurons = 12,\n",
    "        action_space = env.action_space,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    critic_network = CriticNetwork(\n",
    "        nb_neurons = 12,\n",
    "        observation_space = env.observation_space\n",
    "    )\n",
    "    caclavar = CACLAVAR(\n",
    "        learning_rate_critic = 0.01,\n",
    "        learning_rate_actor = 0.01,\n",
    "        discount_factor = 0.99,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_min = 0.01,\n",
    "        epsilon_decay = 0.01,\n",
    "        sigma = 0.1,\n",
    "        nb_episode = 500,\n",
    "        nb_tests = 3,\n",
    "        test_frequency = 1,\n",
    "        env = env,\n",
    "        actor_network = actor_network,\n",
    "        critic_network = critic_network,\n",
    "        exploration_strategy = \"egreedy\",\n",
    "        verbose_mode = False\n",
    "    )\n",
    "    caclavar.learning()\n",
    "    matrice_simulation_rewards_caclavar.append(caclavar.list_rewards_mean)\n",
    "    matrice_simulation_iteration_caclavar.append(caclavar.list_iteration)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    nb_iter = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype=torch.float32)\n",
    "        action =  caclavar.best_model(state_t).detach().numpy()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        state = new_state\n",
    "        nb_iter += 1\n",
    "    \n",
    "    if nb_iter == env.max_iteration :\n",
    "        fails_caclavar += 1\n",
    "    else :\n",
    "        success_caclavar += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4726b3-2ab4-47a9-99c9-3b8135570590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[egreedy] Nombre de tests : {nb_tests}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CAC: {success_cac}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CAC: {fails_cac}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CAC: {success_cac/(success_cac+fails_cac)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CACLA: {success_cacla}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CACLA: {fails_cacla}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CACLA: {success_cacla/(success_cacla+fails_cacla)*100}%\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"[egreedy] Nombre de succes CACLAVAR: {success_caclavar}\")\n",
    "print(f\"[egreedy] Nombre d'echecs CACLAVAR: {fails_caclavar}\")\n",
    "print(f\"[egreedy] Ratio de succes pour CACLAVAR: {success_caclavar/(success_caclavar+fails_caclavar)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056be631-bfbf-413f-be7e-d92efab18d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_normalization(matrice_simulation_rewards) :\n",
    "    dist_max = -200\n",
    "    dist_min = 0\n",
    "    arr = np.array(matrice_simulation_rewards)\n",
    "    # return 1 - ( arr / (dist_max - dist_min)).mean(axis=0)\n",
    "    return arr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb7485-e4a6-432d-b74a-496266a457a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cac = rewards_normalization(matrice_simulation_rewards_cac)\n",
    "l_cacla = rewards_normalization(matrice_simulation_rewards_cacla)\n",
    "l_caclavar = rewards_normalization(matrice_simulation_rewards_caclavar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85596a-b81d-4fb9-ab00-7afd4b810c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_iter_caclavar = np.array(matrice_simulation_iteration_caclavar)\n",
    "x_iter_caclavar = m_iter_caclavar.mean(axis=0)\n",
    "\n",
    "m_iter_cacla = np.array(matrice_simulation_iteration_cacla)\n",
    "x_iter_cacla = m_iter_cacla.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63324fc-a8f3-4eaa-ac32-24054faafccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on CartPole Continuous\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cac, label=\"rewards CAC\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on CartPole Continuous\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,11))\n",
    "plt.title(f\"evolution of the rewards with {nb_tests} simulations and egreedy exploration on CartPole Continuous\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"rewards normalized between 0 and 1\")\n",
    "plt.plot(x_iter_cacla,l_cacla, label=\"rewards CACLA\")\n",
    "plt.plot(x_iter_caclavar,l_caclavar, label=\"rewards CACLAVAR\")\n",
    "plt.xticks([0,51200,102400])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001254c5-6e55-4280-a974-abda8a87cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultats : \")\n",
    "print(\"[egreedy] CAC : mean rewards -> \",l_cac.mean())\n",
    "print(\"[egreedy] CAC : std rewards -> \",l_cac.std())\n",
    "print(\"[egreedy] CACLA : mean rewards -> \",l_cacla.mean())\n",
    "print(\"[egreedy] CACLA : std rewards -> \",l_cacla.std())\n",
    "print(\"[egreedy] CACLAVAR : mean rewards -> \",l_caclavar.mean())\n",
    "print(\"[egreedy] CACLAVAR : std rewards -> \",l_caclavar.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947991d3-b910-45ab-8607-881b507093ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
