{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6037b847-30b5-4928-ad6c-b9674ab68928",
   "metadata": {},
   "source": [
    "# IAR : Mini-projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360251cf-30d6-42ee-8ad4-e552acdeedab",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons implémenter le Double Dueling Deep Q Network sur l'environnement discret de LunarLander. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925bdad-855f-4f85-bb05-1159f68787cd",
   "metadata": {},
   "source": [
    "Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca4ee0c-7589-4f75-b11a-a255390085a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c8f88-6294-400d-a165-f1996a22511b",
   "metadata": {},
   "source": [
    "Définition de l'environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c156e2a-692a-494d-bad4-01cbbe131aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "nb_actions = 4\n",
    "nb_observations = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eda459-5e69-4525-8592-6347195e0f4f",
   "metadata": {},
   "source": [
    "Hyper paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "535c0d0e-6c55-4e12-9ea8-f9e2e9861acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episode = 1000\n",
    "discount_factor = 0.99\n",
    "learning_rate = 2e-3\n",
    "test_frequency = 10\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.02\n",
    "batch_size = 64\n",
    "size_replay_buffer = int(1e5)\n",
    "update_frequency = 1\n",
    "tau = 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5521d696-18b6-4128-b391-8b6e6c4bb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(q_network) :\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cum_sum = 0\n",
    "    while not done :\n",
    "        state_t = torch.as_tensor(state , dtype = torch.float32).unsqueeze(0)\n",
    "        action = torch.argmax(q_network(state_t)).item()\n",
    "        new_state,reward,done,_ = env.step(action)\n",
    "        state = new_state\n",
    "        cum_sum += reward\n",
    "        \n",
    "    return cum_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383065a6-96d2-4c1e-89fb-9c2210621585",
   "metadata": {},
   "source": [
    "Dueling network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596205ab-e893-4414-a3ad-60be80020b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module) :\n",
    "    \n",
    "    def __init__(self,\n",
    "              nb_actions,\n",
    "              nb_observations) : \n",
    "        \n",
    "        super().__init__()\n",
    "        self.nb_actions = nb_actions\n",
    "        self.nb_observations = nb_observations\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(nb_observations,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32)\n",
    "        )\n",
    "        \n",
    "        self.net_advantage = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,nb_actions)\n",
    "        )\n",
    "        \n",
    "        self.net_state_value = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "        \n",
    "    def advantage(self,x) :\n",
    "        return self.net_advantage(self.net(x))\n",
    "    \n",
    "    def state_value(self,x) :\n",
    "        return self.net_state_value(self.net(x))\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        return self.state_value(x) + self.advantage(x) - torch.mean(self.advantage(x),dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb2f23-c58c-471d-9893-7b4511629e18",
   "metadata": {},
   "source": [
    "Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "788bc5ac-e31d-4d3e-9180-8d92ab25af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=size_replay_buffer)\n",
    "q_network = DuelingQNetwork(nb_actions,nb_observations)\n",
    "q_target_network = DuelingQNetwork(nb_actions,nb_observations)\n",
    "q_target_network.load_state_dict(q_network.state_dict())\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "list_tests = []\n",
    "timestep = 0\n",
    "\n",
    "bestModel = DuelingQNetwork(nb_actions,nb_observations)\n",
    "bestModel.load_state_dict(q_network.state_dict())\n",
    "bestvalue = -1e9\n",
    "\n",
    "average_list = deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa6cf9-f6d2-4828-9cd6-809af6ca32c4",
   "metadata": {},
   "source": [
    "Boucle d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bac0256-b571-4ff6-b223-54c2d521d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 1/1000 [00:00<09:55,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 - test reward : -604.8086591419683 - avg : -425.0760512886622 - epsilon 0.8183201210226743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                        | 11/1000 [00:09<21:07,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 10 - test reward : -313.0537783987375 - avg : -187.18711703714555 - epsilon 0.778312557068642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                        | 21/1000 [00:22<35:05,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 20 - test reward : -366.65616540518823 - avg : -192.46175609026946 - epsilon 0.7402609576967045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                     | 31/1000 [00:47<1:26:57,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 30 - test reward : -296.83406273958974 - avg : -177.38085392426993 - epsilon 0.7040696960536299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                       | 41/1000 [00:59<30:56,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 40 - test reward : -155.4424517579527 - avg : -170.66586292585492 - epsilon 0.6696478204705644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                       | 51/1000 [01:14<41:36,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 50 - test reward : -149.4731197355277 - avg : -165.5022635312931 - epsilon 0.6369088258938781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                    | 61/1000 [01:34<1:03:55,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 60 - test reward : -179.70265045428374 - avg : -151.8385621766675 - epsilon 0.6057704364907278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                    | 71/1000 [02:03<1:44:25,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 70 - test reward : -140.05058016721318 - avg : -142.3065970075292 - epsilon 0.5761543988830038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                   | 81/1000 [02:30<1:32:54,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 80 - test reward : -96.85483654351125 - avg : -132.7211756471315 - epsilon 0.547986285490042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▌                                   | 91/1000 [02:55<1:19:38,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 90 - test reward : -137.93626609422407 - avg : -127.689449734903 - epsilon 0.5211953074858876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                  | 101/1000 [03:44<2:20:47,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 - test reward : -154.24495777143144 - avg : -121.40244400437479 - epsilon 0.49571413690105054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▏                                 | 111/1000 [04:49<2:37:15, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 110 - test reward : -168.24886912553535 - avg : -110.38462250371684 - epsilon 0.47147873742168567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▌                                 | 121/1000 [05:19<1:33:19,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 120 - test reward : -135.3231338244661 - avg : -91.80862843324056 - epsilon 0.4484282034609769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████                                   | 126/1000 [05:44<39:50,  2.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13760/1710757807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13760/3089265561.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13760/3089265561.py\u001b[0m in \u001b[0;36madvantage\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_advantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in tqdm(range(nb_episode)) :\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    cumul = 0\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    \n",
    "    while not done : \n",
    "        state_t = torch.as_tensor(state , dtype = torch.float32).unsqueeze(0)\n",
    "        \n",
    "        if random.random() > epsilon :\n",
    "            action = torch.argmax(q_network(state_t)).item()\n",
    "        else :\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state,reward,done,_ = env.step(action)\n",
    "\n",
    "        cumul += reward\n",
    "        \n",
    "        transition = (state,action,done,reward,new_state)\n",
    "        replay_buffer.append(transition)\n",
    "        \n",
    "        if len(replay_buffer) >= batch_size and timestep % update_frequency == 0 :\n",
    "        \n",
    "            batch = random.sample(replay_buffer,batch_size)\n",
    "\n",
    "            states = np.asarray([exp[0] for exp in batch],dtype=np.float32)\n",
    "            actions = np.asarray([exp[1] for exp in batch],dtype=int)\n",
    "            dones = np.asarray([exp[2] for exp in batch],dtype=int)\n",
    "            rewards = np.asarray([exp[3] for exp in batch],dtype=np.float32)\n",
    "            new_states = np.asarray([exp[4] for exp in batch],dtype=np.float32)\n",
    "            \n",
    "            states_t = torch.as_tensor(states , dtype=torch.float32)\n",
    "            dones_t = torch.as_tensor(dones , dtype = torch.int64).unsqueeze(1)\n",
    "            new_states_t = torch.as_tensor(new_states , dtype=torch.float32)\n",
    "            actions_t = torch.as_tensor(actions , dtype = torch.int64).unsqueeze(1)\n",
    "            rewards_t = torch.as_tensor(rewards , dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "            \n",
    "            y_target = rewards_t + discount_factor * (1 - dones_t) * torch.gather(q_target_network(new_states_t),dim=1,index=torch.argmax(q_network(new_states_t),dim=1).unsqueeze(1)).detach()\n",
    "\n",
    "            mse = nn.MSELoss()\n",
    "\n",
    "            loss = mse(torch.gather(q_network(states_t),dim=1,index=actions_t), y_target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            for target_param, local_param in zip(q_target_network.parameters(), q_network.parameters()):\n",
    "                target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)  \n",
    "                \n",
    "        timestep += 1\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "    \n",
    "    average_list.append(cumul)\n",
    "    \n",
    "    if episode % test_frequency == 0 :\n",
    "        t =  0\n",
    "        for _ in range(10) :\n",
    "            t += test(q_network)\n",
    "        t /= 10\n",
    "        if t > bestvalue :\n",
    "            bestvalue = t\n",
    "            bestModel.load_state_dict(q_network.state_dict())\n",
    "        avg = sum(average_list) / len(average_list)\n",
    "        print(f\"episode {episode} - test reward : {t} - avg : {avg} - epsilon {epsilon}\")\n",
    "        list_tests.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ec7d2-15bb-4b94-b122-0dfc3ea231c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bestModel.state_dict(),\"best_model_dq3n_lunarlanderdiscret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc3d60-d22d-4d1c-b98d-62273dda560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"lunarlander - dq3n - rewards\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.plot(np.arange(0,nb_episode,test_frequency),list_tests)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
